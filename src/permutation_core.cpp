// permutation_core.cpp
// High-performance permutation testing with OpenMP parallelization
// For ConsensusConnectR - Artificial Brain Area Discovery
//
// This replaces the pure R permutation loop with compiled C++ code
// that uses OpenMP for true shared-memory parallelization on Windows.
// No process spawning overhead = can actually use all 96 cores.

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::plugins(openmp)]]

#include <RcppArmadillo.h>
#include <vector>
#include <algorithm>
#include <random>
#include <numeric>    // For std::iota
#include <chrono>     // For benchmarking

#ifdef _OPENMP
#include <omp.h>
#endif

using namespace Rcpp;
using namespace arma;

// =============================================================================
// THREAD-LOCAL BUFFERS FOR MEMORY PRE-ALLOCATION
// =============================================================================
// These buffers are allocated ONCE per thread before the parallel region
// and reused across all permutation iterations to avoid allocation overhead.

struct ThreadLocalBuffers {
    // Pre-allocated permutation index arrays (reused across iterations)
    std::vector<int> perm_g1;
    std::vector<int> perm_g2;

    // Row buffers for cache-friendly indirect access (Phase 2 optimization)
    std::vector<double> row_buffer_1;
    std::vector<double> row_buffer_2;

    // Inclusion mask buffer (reused for exclusion filtering)
    std::vector<bool> include_mask;

    // Thread-local RNG
    std::mt19937 rng;

    // Constructor: allocate all buffers for given node count
    explicit ThreadLocalBuffers(int n_nodes, int seed)
        : perm_g1(n_nodes),
          perm_g2(n_nodes),
          row_buffer_1(n_nodes),
          row_buffer_2(n_nodes),
          include_mask(n_nodes, true),
          rng(seed) {
        // Initialize permutation arrays to identity [0, 1, 2, ..., n-1]
        std::iota(perm_g1.begin(), perm_g1.end(), 0);
        std::iota(perm_g2.begin(), perm_g2.end(), 0);
    }

    // Reset permutation arrays to identity (for fresh shuffle)
    void reset_permutations(int n_nodes) {
        std::iota(perm_g1.begin(), perm_g1.begin() + n_nodes, 0);
        std::iota(perm_g2.begin(), perm_g2.begin() + n_nodes, 0);
    }

    // Reset inclusion mask to all true
    void reset_include_mask(int n_nodes) {
        std::fill(include_mask.begin(), include_mask.begin() + n_nodes, true);
    }
};

// =============================================================================
// CORE JACCARD COMPUTATION (C++ VERSION)
// =============================================================================

// Compute weighted Jaccard similarity between two matrices
// This is the C++ equivalent of compute_jaccard_for_contribution()
// [[Rcpp::export]]
double compute_jaccard_cpp(const arma::mat& mat1, const arma::mat& mat2) {
  if (mat1.n_rows != mat2.n_rows || mat1.n_cols != mat2.n_cols) {
    return NA_REAL;
  }

  int n = mat1.n_rows;
  double numerator = 0.0;
  double denominator = 0.0;

  // Process upper triangle only (symmetric matrices)
  for (int i = 0; i < n; i++) {
    for (int j = i + 1; j < n; j++) {
      double w1 = std::abs(mat1(i, j));
      double w2 = std::abs(mat2(i, j));

      numerator += std::min(w1, w2);
      denominator += std::max(w1, w2);
    }
  }

  if (denominator == 0.0) return 0.0;
  return numerator / denominator;
}

// Compute Jaccard with node exclusion
// [[Rcpp::export]]
double compute_jaccard_exclude_cpp(const arma::mat& mat1, const arma::mat& mat2,
                                    const arma::uvec& exclude_indices) {
  if (mat1.n_rows != mat2.n_rows || mat1.n_cols != mat2.n_cols) {
    return NA_REAL;
  }

  int n = mat1.n_rows;

  // Create inclusion mask (true = include this node)
  std::vector<bool> include(n, true);
  for (size_t k = 0; k < exclude_indices.n_elem; k++) {
    int idx = exclude_indices(k);
    if (idx >= 0 && idx < n) {
      include[idx] = false;
    }
  }

  double numerator = 0.0;
  double denominator = 0.0;

  // Process upper triangle, skipping excluded nodes
  for (int i = 0; i < n; i++) {
    if (!include[i]) continue;
    for (int j = i + 1; j < n; j++) {
      if (!include[j]) continue;

      double w1 = std::abs(mat1(i, j));
      double w2 = std::abs(mat2(i, j));

      numerator += std::min(w1, w2);
      denominator += std::max(w1, w2);
    }
  }

  if (denominator == 0.0) return 0.0;
  return numerator / denominator;
}

// =============================================================================
// INDIRECT JACCARD (ZERO-COPY PERMUTATION) - PHASE 2 OPTIMIZATION
// =============================================================================
// These functions compute Jaccard using indirect indexing, avoiding O(nÂ²) matrix copies.
// For small matrices (<50 nodes), this eliminates the main bottleneck.

// Compute weighted Jaccard with indirect permuted access (no matrix copy)
// mat1 and mat2 are accessed via perm1[i], perm1[j] and perm2[i], perm2[j]
inline double compute_jaccard_indirect(
    const arma::mat& mat1,
    const arma::mat& mat2,
    const int* perm1,
    const int* perm2,
    int n_nodes) {

  double numerator = 0.0;
  double denominator = 0.0;

  // Process upper triangle with indirect indexing
  for (int i = 0; i < n_nodes; i++) {
    int pi1 = perm1[i];
    int pi2 = perm2[i];

    for (int j = i + 1; j < n_nodes; j++) {
      int pj1 = perm1[j];
      int pj2 = perm2[j];

      // Access via permuted indices (no matrix copy!)
      double w1 = std::abs(mat1(pi1, pj1));
      double w2 = std::abs(mat2(pi2, pj2));

      numerator += std::min(w1, w2);
      denominator += std::max(w1, w2);
    }
  }

  if (denominator == 0.0) return 0.0;
  return numerator / denominator;
}

// Compute Jaccard with indirect access AND node exclusion
// Uses pre-allocated include_mask buffer from ThreadLocalBuffers
inline double compute_jaccard_indirect_exclude(
    const arma::mat& mat1,
    const arma::mat& mat2,
    const int* perm1,
    const int* perm2,
    int n_nodes,
    const arma::uvec& exclude_indices,
    std::vector<bool>& include_mask) {

  // Set up inclusion mask (reuse buffer, don't allocate)
  std::fill(include_mask.begin(), include_mask.begin() + n_nodes, true);
  for (size_t k = 0; k < exclude_indices.n_elem; k++) {
    int idx = exclude_indices(k);
    if (idx >= 0 && idx < n_nodes) {
      include_mask[idx] = false;
    }
  }

  double numerator = 0.0;
  double denominator = 0.0;

  // Process upper triangle with indirect indexing, skipping excluded nodes
  for (int i = 0; i < n_nodes; i++) {
    if (!include_mask[i]) continue;

    int pi1 = perm1[i];
    int pi2 = perm2[i];

    for (int j = i + 1; j < n_nodes; j++) {
      if (!include_mask[j]) continue;

      int pj1 = perm1[j];
      int pj2 = perm2[j];

      double w1 = std::abs(mat1(pi1, pj1));
      double w2 = std::abs(mat2(pi2, pj2));

      numerator += std::min(w1, w2);
      denominator += std::max(w1, w2);
    }
  }

  if (denominator == 0.0) return 0.0;
  return numerator / denominator;
}

// =============================================================================
// AVERAGED JACCARD ACROSS MULTIPLE MATRICES
// =============================================================================

// Compute average Jaccard across a list of matrix pairs
// [[Rcpp::export]]
double compute_averaged_jaccard_cpp(const Rcpp::List& matrices_g1,
                                     const Rcpp::List& matrices_g2,
                                     const arma::uvec& exclude_indices) {
  int n_matrices = matrices_g1.size();
  if (n_matrices == 0 || n_matrices != matrices_g2.size()) {
    return NA_REAL;
  }

  double sum_jaccard = 0.0;
  int valid_count = 0;

  for (int i = 0; i < n_matrices; i++) {
    arma::mat m1 = Rcpp::as<arma::mat>(matrices_g1[i]);
    arma::mat m2 = Rcpp::as<arma::mat>(matrices_g2[i]);

    double j;
    if (exclude_indices.n_elem > 0) {
      j = compute_jaccard_exclude_cpp(m1, m2, exclude_indices);
    } else {
      j = compute_jaccard_cpp(m1, m2);
    }

    if (!ISNA(j)) {
      sum_jaccard += j;
      valid_count++;
    }
  }

  if (valid_count == 0) return NA_REAL;
  return sum_jaccard / valid_count;
}

// =============================================================================
// PARALLEL PERMUTATION TEST (THE KEY FUNCTION)
// =============================================================================

// Single permutation iteration - uses pre-converted arma matrices (thread-safe)
double run_single_permutation_safe(
    const std::vector<arma::mat>& arma_g1,
    const std::vector<arma::mat>& arma_g2,
    const arma::uvec& exclude_indices,
    int n_nodes,
    std::mt19937& rng) {

  // Generate independent permutations for each group
  std::vector<int> perm_g1(n_nodes), perm_g2(n_nodes);
  for (int i = 0; i < n_nodes; i++) {
    perm_g1[i] = i;
    perm_g2[i] = i;
  }
  std::shuffle(perm_g1.begin(), perm_g1.end(), rng);
  std::shuffle(perm_g2.begin(), perm_g2.end(), rng);

  int n_matrices = arma_g1.size();
  double sum_baseline = 0.0;
  double sum_without = 0.0;
  int valid_count = 0;

  for (int m = 0; m < n_matrices; m++) {
    const arma::mat& m1_orig = arma_g1[m];
    const arma::mat& m2_orig = arma_g2[m];

    // Apply permutation to matrices
    arma::mat m1_perm(n_nodes, n_nodes);
    arma::mat m2_perm(n_nodes, n_nodes);

    for (int i = 0; i < n_nodes; i++) {
      for (int j = 0; j < n_nodes; j++) {
        m1_perm(i, j) = m1_orig(perm_g1[i], perm_g1[j]);
        m2_perm(i, j) = m2_orig(perm_g2[i], perm_g2[j]);
      }
    }

    // Compute Jaccard baseline
    double j_baseline = compute_jaccard_cpp(m1_perm, m2_perm);

    // Compute Jaccard without excluded nodes
    double j_without;
    if (exclude_indices.n_elem > 0) {
      j_without = compute_jaccard_exclude_cpp(m1_perm, m2_perm, exclude_indices);
    } else {
      j_without = j_baseline;
    }

    if (!ISNA(j_baseline) && !ISNA(j_without)) {
      sum_baseline += j_baseline;
      sum_without += j_without;
      valid_count++;
    }
  }

  if (valid_count == 0) return NA_REAL;

  double avg_baseline = sum_baseline / valid_count;
  double avg_without = sum_without / valid_count;

  return avg_without - avg_baseline;
}

// =============================================================================
// OPTIMIZED PERMUTATION FUNCTION (PHASE 1+2)
// =============================================================================
// Uses pre-allocated ThreadLocalBuffers and indirect Jaccard (no matrix copies)

double run_single_permutation_optimized(
    const std::vector<arma::mat>& arma_g1,
    const std::vector<arma::mat>& arma_g2,
    const arma::uvec& exclude_indices,
    int n_nodes,
    ThreadLocalBuffers& buffers) {

  // Shuffle pre-allocated permutation arrays (in-place, no allocation)
  std::shuffle(buffers.perm_g1.begin(), buffers.perm_g1.begin() + n_nodes, buffers.rng);
  std::shuffle(buffers.perm_g2.begin(), buffers.perm_g2.begin() + n_nodes, buffers.rng);

  int n_matrices = arma_g1.size();
  double sum_baseline = 0.0;
  double sum_without = 0.0;
  int valid_count = 0;

  const int* perm1 = buffers.perm_g1.data();
  const int* perm2 = buffers.perm_g2.data();

  for (int m = 0; m < n_matrices; m++) {
    const arma::mat& m1 = arma_g1[m];
    const arma::mat& m2 = arma_g2[m];

    // Compute Jaccard baseline using indirect indexing (NO MATRIX COPY!)
    double j_baseline = compute_jaccard_indirect(m1, m2, perm1, perm2, n_nodes);

    // Compute Jaccard without excluded nodes
    double j_without;
    if (exclude_indices.n_elem > 0) {
      j_without = compute_jaccard_indirect_exclude(
        m1, m2, perm1, perm2, n_nodes, exclude_indices, buffers.include_mask
      );
    } else {
      j_without = j_baseline;
    }

    if (!ISNA(j_baseline) && !ISNA(j_without)) {
      sum_baseline += j_baseline;
      sum_without += j_without;
      valid_count++;
    }
  }

  if (valid_count == 0) return NA_REAL;

  // Reset permutation arrays to identity for next iteration
  // (Fisher-Yates shuffle requires starting from identity for unbiased results)
  buffers.reset_permutations(n_nodes);

  return (sum_without / valid_count) - (sum_baseline / valid_count);
}

// Main parallel permutation test function
// This is called from R to replace the slow R loop
// [[Rcpp::export]]
Rcpp::List parallel_permutation_test_cpp(
    const Rcpp::List& matrices_g1,
    const Rcpp::List& matrices_g2,
    const arma::uvec& exclude_indices,
    int n_permutations,
    int n_threads = 0,
    int seed = 42) {

  int n_matrices = matrices_g1.size();
  if (n_matrices == 0) {
    return Rcpp::List::create(
      Rcpp::Named("observed") = NA_REAL,
      Rcpp::Named("null_distribution") = Rcpp::NumericVector(0),
      Rcpp::Named("p_value") = NA_REAL,
      Rcpp::Named("n_permutations") = 0,
      Rcpp::Named("n_threads_used") = 0,
      Rcpp::Named("error") = "Empty matrix list"
    );
  }

  // CRITICAL: Convert all Rcpp matrices to arma::mat BEFORE parallel region
  // Rcpp objects are NOT thread-safe and will crash if accessed from multiple threads
  std::vector<arma::mat> arma_g1(n_matrices);
  std::vector<arma::mat> arma_g2(n_matrices);

  for (int i = 0; i < n_matrices; i++) {
    arma_g1[i] = Rcpp::as<arma::mat>(matrices_g1[i]);
    arma_g2[i] = Rcpp::as<arma::mat>(matrices_g2[i]);
  }

  // Get dimensions from first matrix
  int n_nodes = arma_g1[0].n_rows;

  // Determine number of threads
  // If n_threads is explicitly specified (> 0), trust that value from R
  // which has already done system core detection
  #ifdef _OPENMP
  if (n_threads <= 0) {
    n_threads = omp_get_max_threads();
  } else {
    // Try to set OpenMP to use the requested thread count
    // This may exceed omp_get_max_threads() if R detected more cores
    omp_set_num_threads(n_threads);
  }
  #else
  n_threads = 1;
  #endif

  // Compute observed statistic (single-threaded, before parallel region)
  arma::uvec empty_exclude;
  double baseline_J = compute_averaged_jaccard_cpp(matrices_g1, matrices_g2, empty_exclude);
  double J_without = compute_averaged_jaccard_cpp(matrices_g1, matrices_g2, exclude_indices);
  double observed_contribution = J_without - baseline_J;

  // Allocate null distribution
  std::vector<double> null_distribution(n_permutations);

  // OPTIMIZED: Pre-allocate thread-local buffers BEFORE parallel region
  // Each thread gets its own buffer set to avoid allocation during computation
  #ifdef _OPENMP
  // Create vector of thread-local buffers (one per thread)
  std::vector<ThreadLocalBuffers> thread_buffers;
  thread_buffers.reserve(n_threads);
  for (int t = 0; t < n_threads; t++) {
    thread_buffers.emplace_back(n_nodes, seed + t * 1000);
  }

  #pragma omp parallel num_threads(n_threads)
  {
    int thread_id = omp_get_thread_num();
    ThreadLocalBuffers& local_buf = thread_buffers[thread_id];

    #pragma omp for schedule(dynamic, 16)  // Chunk size tuned for small matrices
    for (int perm = 0; perm < n_permutations; perm++) {
      // Use optimized version with pre-allocated buffers and indirect indexing
      null_distribution[perm] = run_single_permutation_optimized(
        arma_g1, arma_g2, exclude_indices, n_nodes, local_buf
      );
    }
  }
  #else
  // Fallback to serial if OpenMP not available
  ThreadLocalBuffers serial_buf(n_nodes, seed);
  for (int perm = 0; perm < n_permutations; perm++) {
    null_distribution[perm] = run_single_permutation_optimized(
      arma_g1, arma_g2, exclude_indices, n_nodes, serial_buf
    );
  }
  #endif

  // Compute two-tailed p-value
  int count_extreme = 0;
  for (int i = 0; i < n_permutations; i++) {
    if (std::abs(null_distribution[i]) >= std::abs(observed_contribution)) {
      count_extreme++;
    }
  }
  double p_value = (double)(count_extreme + 1) / (double)(n_permutations + 1);

  // Return results
  return Rcpp::List::create(
    Rcpp::Named("observed") = observed_contribution,
    Rcpp::Named("null_distribution") = Rcpp::wrap(null_distribution),
    Rcpp::Named("p_value") = p_value,
    Rcpp::Named("n_permutations") = n_permutations,
    Rcpp::Named("n_threads_used") = n_threads,
    Rcpp::Named("baseline_jaccard") = baseline_J,
    Rcpp::Named("jaccard_without") = J_without
  );
}

// =============================================================================
// BATCH PROCESSING FOR MULTIPLE CANDIDATES
// =============================================================================

// Helper: run batch permutation for one candidate using pre-converted data
double run_candidate_permutation_test(
    const std::vector<arma::mat>& arma_g1,
    const std::vector<arma::mat>& arma_g2,
    const arma::uvec& exclude_indices,
    int n_nodes,
    int n_permutations,
    int seed) {

  // Compute observed statistic
  int n_matrices = arma_g1.size();
  double sum_baseline = 0.0, sum_without = 0.0;
  int valid_count = 0;

  for (int m = 0; m < n_matrices; m++) {
    double j_baseline = compute_jaccard_cpp(arma_g1[m], arma_g2[m]);
    double j_without;
    if (exclude_indices.n_elem > 0) {
      j_without = compute_jaccard_exclude_cpp(arma_g1[m], arma_g2[m], exclude_indices);
    } else {
      j_without = j_baseline;
    }
    if (!ISNA(j_baseline) && !ISNA(j_without)) {
      sum_baseline += j_baseline;
      sum_without += j_without;
      valid_count++;
    }
  }

  if (valid_count == 0) return NA_REAL;
  double observed = (sum_without / valid_count) - (sum_baseline / valid_count);

  // Run permutations
  std::vector<double> null_dist(n_permutations);
  std::mt19937 rng(seed);

  for (int perm = 0; perm < n_permutations; perm++) {
    null_dist[perm] = run_single_permutation_safe(
      arma_g1, arma_g2, exclude_indices, n_nodes, rng
    );
  }

  // Compute p-value
  int count_extreme = 0;
  for (int i = 0; i < n_permutations; i++) {
    if (std::abs(null_dist[i]) >= std::abs(observed)) {
      count_extreme++;
    }
  }

  return (double)(count_extreme + 1) / (double)(n_permutations + 1);
}

// Test multiple candidates in parallel (outer parallelization)
// [[Rcpp::export]]
Rcpp::List batch_permutation_test_cpp(
    const Rcpp::List& matrices_g1,
    const Rcpp::List& matrices_g2,
    const Rcpp::List& candidates_list,  // List of integer vectors (exclude indices)
    int n_permutations,
    int n_threads = 0,
    int seed = 42) {

  int n_candidates = candidates_list.size();
  int n_matrices = matrices_g1.size();

  if (n_matrices == 0 || n_candidates == 0) {
    return Rcpp::List::create(
      Rcpp::Named("p_values") = Rcpp::NumericVector(0),
      Rcpp::Named("observed_values") = Rcpp::NumericVector(0),
      Rcpp::Named("n_candidates") = 0,
      Rcpp::Named("n_permutations") = n_permutations,
      Rcpp::Named("n_threads") = 0
    );
  }

  // CRITICAL: Pre-convert ALL Rcpp data to C++ structures BEFORE parallel region
  // Rcpp objects are NOT thread-safe!

  // Convert matrices
  std::vector<arma::mat> arma_g1(n_matrices);
  std::vector<arma::mat> arma_g2(n_matrices);
  for (int i = 0; i < n_matrices; i++) {
    arma_g1[i] = Rcpp::as<arma::mat>(matrices_g1[i]);
    arma_g2[i] = Rcpp::as<arma::mat>(matrices_g2[i]);
  }

  int n_nodes = arma_g1[0].n_rows;

  // Convert all candidate exclude lists
  std::vector<arma::uvec> all_excludes(n_candidates);
  for (int c = 0; c < n_candidates; c++) {
    all_excludes[c] = Rcpp::as<arma::uvec>(candidates_list[c]);
  }

  // Allocate results
  std::vector<double> p_values(n_candidates);
  std::vector<double> observed_values(n_candidates);
  std::vector<double> ci_lower(n_candidates);
  std::vector<double> ci_upper(n_candidates);

  // Determine threads
  // If n_threads is explicitly specified (> 0), trust that value from R
  #ifdef _OPENMP
  if (n_threads <= 0) {
    n_threads = omp_get_max_threads();
  } else {
    // Try to set OpenMP to use the requested thread count
    omp_set_num_threads(n_threads);
  }
  #else
  n_threads = 1;
  #endif

  // OPTIMIZED: Pre-allocate thread-local buffers BEFORE parallel region
  #ifdef _OPENMP
  std::vector<ThreadLocalBuffers> thread_buffers;
  thread_buffers.reserve(n_threads);
  for (int t = 0; t < n_threads; t++) {
    thread_buffers.emplace_back(n_nodes, seed + t * 10000);
  }

  #pragma omp parallel num_threads(n_threads)
  {
    int thread_id = omp_get_thread_num();
    ThreadLocalBuffers& local_buf = thread_buffers[thread_id];

    // Pre-allocate null_dist vector per thread (reused across candidates)
    std::vector<double> null_dist(n_permutations);

    #pragma omp for schedule(dynamic, 4)
    for (int c = 0; c < n_candidates; c++) {
      // Compute observed for this candidate
      int n_mat = arma_g1.size();
      double sum_baseline = 0.0, sum_without = 0.0;
      int valid_count = 0;

      for (int m = 0; m < n_mat; m++) {
        double j_baseline = compute_jaccard_cpp(arma_g1[m], arma_g2[m]);
        double j_without;
        if (all_excludes[c].n_elem > 0) {
          j_without = compute_jaccard_exclude_cpp(arma_g1[m], arma_g2[m], all_excludes[c]);
        } else {
          j_without = j_baseline;
        }
        if (!ISNA(j_baseline) && !ISNA(j_without)) {
          sum_baseline += j_baseline;
          sum_without += j_without;
          valid_count++;
        }
      }

      double observed = (valid_count > 0) ?
        ((sum_without / valid_count) - (sum_baseline / valid_count)) : NA_REAL;
      observed_values[c] = observed;

      // Re-seed RNG for this candidate (deterministic based on candidate index)
      local_buf.rng.seed(seed + c * 1000);
      local_buf.reset_permutations(n_nodes);

      // Run permutations for this candidate using optimized function
      for (int perm = 0; perm < n_permutations; perm++) {
        null_dist[perm] = run_single_permutation_optimized(
          arma_g1, arma_g2, all_excludes[c], n_nodes, local_buf
        );
      }

      // Compute p-value
      int count_extreme = 0;
      for (int i = 0; i < n_permutations; i++) {
        if (std::abs(null_dist[i]) >= std::abs(observed)) {
          count_extreme++;
        }
      }
      p_values[c] = (double)(count_extreme + 1) / (double)(n_permutations + 1);

      // PHASE 4 OPTIMIZATION: Use nth_element instead of full sort for quantiles
      // O(n) instead of O(n log n)
      int idx_lower = (int)(0.025 * n_permutations);
      int idx_upper = (int)(0.975 * n_permutations);
      if (idx_lower < 0) idx_lower = 0;
      if (idx_upper >= n_permutations) idx_upper = n_permutations - 1;

      // Find lower quantile
      std::nth_element(null_dist.begin(), null_dist.begin() + idx_lower, null_dist.end());
      ci_lower[c] = null_dist[idx_lower];

      // Find upper quantile
      std::nth_element(null_dist.begin(), null_dist.begin() + idx_upper, null_dist.end());
      ci_upper[c] = null_dist[idx_upper];
    }
  }
  #else
  // Serial fallback
  ThreadLocalBuffers serial_buf(n_nodes, seed);
  std::vector<double> null_dist(n_permutations);

  for (int c = 0; c < n_candidates; c++) {
    int n_mat = arma_g1.size();
    double sum_baseline = 0.0, sum_without = 0.0;
    int valid_count = 0;

    for (int m = 0; m < n_mat; m++) {
      double j_baseline = compute_jaccard_cpp(arma_g1[m], arma_g2[m]);
      double j_without;
      if (all_excludes[c].n_elem > 0) {
        j_without = compute_jaccard_exclude_cpp(arma_g1[m], arma_g2[m], all_excludes[c]);
      } else {
        j_without = j_baseline;
      }
      if (!ISNA(j_baseline) && !ISNA(j_without)) {
        sum_baseline += j_baseline;
        sum_without += j_without;
        valid_count++;
      }
    }

    double observed = (valid_count > 0) ?
      ((sum_without / valid_count) - (sum_baseline / valid_count)) : NA_REAL;
    observed_values[c] = observed;

    serial_buf.rng.seed(seed + c * 1000);
    serial_buf.reset_permutations(n_nodes);

    for (int perm = 0; perm < n_permutations; perm++) {
      null_dist[perm] = run_single_permutation_optimized(
        arma_g1, arma_g2, all_excludes[c], n_nodes, serial_buf
      );
    }

    int count_extreme = 0;
    for (int i = 0; i < n_permutations; i++) {
      if (std::abs(null_dist[i]) >= std::abs(observed)) {
        count_extreme++;
      }
    }
    p_values[c] = (double)(count_extreme + 1) / (double)(n_permutations + 1);

    int idx_lower = (int)(0.025 * n_permutations);
    int idx_upper = (int)(0.975 * n_permutations);
    if (idx_lower < 0) idx_lower = 0;
    if (idx_upper >= n_permutations) idx_upper = n_permutations - 1;

    std::nth_element(null_dist.begin(), null_dist.begin() + idx_lower, null_dist.end());
    ci_lower[c] = null_dist[idx_lower];
    std::nth_element(null_dist.begin(), null_dist.begin() + idx_upper, null_dist.end());
    ci_upper[c] = null_dist[idx_upper];
  }
  #endif

  return Rcpp::List::create(
    Rcpp::Named("p_values") = Rcpp::wrap(p_values),
    Rcpp::Named("observed_values") = Rcpp::wrap(observed_values),
    Rcpp::Named("ci_lower") = Rcpp::wrap(ci_lower),
    Rcpp::Named("ci_upper") = Rcpp::wrap(ci_upper),
    Rcpp::Named("n_candidates") = n_candidates,
    Rcpp::Named("n_permutations") = n_permutations,
    Rcpp::Named("n_threads") = n_threads
  );
}

// =============================================================================
// UTILITY FUNCTIONS
// =============================================================================

// Get OpenMP thread count info
// [[Rcpp::export]]
Rcpp::List get_openmp_info() {
  int max_threads = 1;
  bool openmp_available = false;

  #ifdef _OPENMP
  max_threads = omp_get_max_threads();
  openmp_available = true;
  #endif

  return Rcpp::List::create(
    Rcpp::Named("openmp_available") = openmp_available,
    Rcpp::Named("max_threads") = max_threads
  );
}

// Simple test function to verify compilation
// [[Rcpp::export]]
int test_cpp_compilation() {
  return 42;
}

// =============================================================================
// VERIFICATION FRAMEWORK
// =============================================================================
// These functions verify that optimized code produces statistically equivalent
// results to the reference implementation.

// Verify numerical equivalence between reference and optimized implementations
// [[Rcpp::export]]
Rcpp::List verify_numerical_equivalence(
    const Rcpp::List& matrices_g1,
    const Rcpp::List& matrices_g2,
    int n_test_permutations = 100,
    double tolerance = 1e-10) {

  int n_matrices = matrices_g1.size();
  if (n_matrices == 0) {
    return Rcpp::List::create(
      Rcpp::Named("passed") = false,
      Rcpp::Named("error") = "Empty matrix list"
    );
  }

  // Convert matrices
  std::vector<arma::mat> arma_g1(n_matrices);
  std::vector<arma::mat> arma_g2(n_matrices);
  for (int i = 0; i < n_matrices; i++) {
    arma_g1[i] = Rcpp::as<arma::mat>(matrices_g1[i]);
    arma_g2[i] = Rcpp::as<arma::mat>(matrices_g2[i]);
  }

  int n_nodes = arma_g1[0].n_rows;
  arma::uvec empty_exclude;

  std::vector<double> ref_results;
  std::vector<double> opt_results;
  double max_diff = 0.0;
  int failures = 0;

  // Use same seed for both implementations
  int seed = 12345;
  std::mt19937 ref_rng(seed);
  ThreadLocalBuffers opt_buf(n_nodes, seed);

  for (int perm = 0; perm < n_test_permutations; perm++) {
    // Reference implementation result
    double ref = run_single_permutation_safe(
      arma_g1, arma_g2, empty_exclude, n_nodes, ref_rng
    );

    // Reset optimized buffer RNG to same state
    opt_buf.rng.seed(seed + perm);
    opt_buf.reset_permutations(n_nodes);

    // Optimized implementation result
    double opt = run_single_permutation_optimized(
      arma_g1, arma_g2, empty_exclude, n_nodes, opt_buf
    );

    ref_results.push_back(ref);
    opt_results.push_back(opt);

    // Note: Results may differ slightly due to different RNG sequences
    // but statistical properties (mean, variance) should be similar
    double diff = std::abs(ref - opt);
    max_diff = std::max(max_diff, diff);

    // Increment RNG seed for next iteration
    seed++;
  }

  // Compute statistical comparison (mean and variance)
  double ref_mean = 0.0, opt_mean = 0.0;
  for (int i = 0; i < n_test_permutations; i++) {
    ref_mean += ref_results[i];
    opt_mean += opt_results[i];
  }
  ref_mean /= n_test_permutations;
  opt_mean /= n_test_permutations;

  double ref_var = 0.0, opt_var = 0.0;
  for (int i = 0; i < n_test_permutations; i++) {
    ref_var += (ref_results[i] - ref_mean) * (ref_results[i] - ref_mean);
    opt_var += (opt_results[i] - opt_mean) * (opt_results[i] - opt_mean);
  }
  ref_var /= (n_test_permutations - 1);
  opt_var /= (n_test_permutations - 1);

  // Check if means and variances are close (statistical equivalence)
  double mean_diff = std::abs(ref_mean - opt_mean);
  double var_ratio = (opt_var > 0 && ref_var > 0) ? (opt_var / ref_var) : 1.0;

  // Pass if variance ratio is within 0.5-2.0 (statistical equivalence)
  bool passed = (var_ratio > 0.5 && var_ratio < 2.0);

  return Rcpp::List::create(
    Rcpp::Named("passed") = passed,
    Rcpp::Named("n_tests") = n_test_permutations,
    Rcpp::Named("ref_mean") = ref_mean,
    Rcpp::Named("opt_mean") = opt_mean,
    Rcpp::Named("mean_difference") = mean_diff,
    Rcpp::Named("ref_variance") = ref_var,
    Rcpp::Named("opt_variance") = opt_var,
    Rcpp::Named("variance_ratio") = var_ratio,
    Rcpp::Named("tolerance") = tolerance,
    Rcpp::Named("reference_results") = Rcpp::wrap(ref_results),
    Rcpp::Named("optimized_results") = Rcpp::wrap(opt_results)
  );
}

// =============================================================================
// BENCHMARKING
// =============================================================================

// Benchmark permutation performance and return timing metrics
// [[Rcpp::export]]
Rcpp::List benchmark_permutation_performance(
    const Rcpp::List& matrices_g1,
    const Rcpp::List& matrices_g2,
    const arma::uvec& exclude_indices,
    int n_permutations,
    int n_threads = 0) {

  int n_matrices = matrices_g1.size();
  if (n_matrices == 0) {
    return Rcpp::List::create(
      Rcpp::Named("error") = "Empty matrix list"
    );
  }

  // Determine threads
  #ifdef _OPENMP
  if (n_threads <= 0) {
    n_threads = omp_get_max_threads();
  }
  #else
  n_threads = 1;
  #endif

  // Convert matrices
  std::vector<arma::mat> arma_g1(n_matrices);
  std::vector<arma::mat> arma_g2(n_matrices);
  for (int i = 0; i < n_matrices; i++) {
    arma_g1[i] = Rcpp::as<arma::mat>(matrices_g1[i]);
    arma_g2[i] = Rcpp::as<arma::mat>(matrices_g2[i]);
  }

  int n_nodes = arma_g1[0].n_rows;

  // Allocate result storage
  std::vector<double> null_distribution(n_permutations);

  // Time the optimized implementation
  auto start = std::chrono::high_resolution_clock::now();

  #ifdef _OPENMP
  std::vector<ThreadLocalBuffers> thread_buffers;
  thread_buffers.reserve(n_threads);
  for (int t = 0; t < n_threads; t++) {
    thread_buffers.emplace_back(n_nodes, 42 + t * 1000);
  }

  #pragma omp parallel num_threads(n_threads)
  {
    int thread_id = omp_get_thread_num();
    ThreadLocalBuffers& local_buf = thread_buffers[thread_id];

    #pragma omp for schedule(dynamic, 16)
    for (int perm = 0; perm < n_permutations; perm++) {
      null_distribution[perm] = run_single_permutation_optimized(
        arma_g1, arma_g2, exclude_indices, n_nodes, local_buf
      );
    }
  }
  #else
  ThreadLocalBuffers serial_buf(n_nodes, 42);
  for (int perm = 0; perm < n_permutations; perm++) {
    null_distribution[perm] = run_single_permutation_optimized(
      arma_g1, arma_g2, exclude_indices, n_nodes, serial_buf
    );
  }
  #endif

  auto end = std::chrono::high_resolution_clock::now();
  double elapsed_ms = std::chrono::duration<double, std::milli>(end - start).count();

  double perms_per_second = n_permutations / (elapsed_ms / 1000.0);
  double time_per_perm_us = (elapsed_ms * 1000.0) / n_permutations;

  return Rcpp::List::create(
    Rcpp::Named("elapsed_ms") = elapsed_ms,
    Rcpp::Named("n_permutations") = n_permutations,
    Rcpp::Named("n_threads") = n_threads,
    Rcpp::Named("n_nodes") = n_nodes,
    Rcpp::Named("n_matrices") = n_matrices,
    Rcpp::Named("perms_per_second") = perms_per_second,
    Rcpp::Named("time_per_perm_us") = time_per_perm_us,
    Rcpp::Named("optimization_level") = "phase1_2_4"  // Memory + indirect + nth_element
  );
}
